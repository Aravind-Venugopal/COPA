{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Install packages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "//anaconda3/lib/python3.7/site-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence, defaultdict\n",
      "//anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import statistics \n",
    "import pickle\n",
    "import jmespath\n",
    "import json\n",
    "from rake_nltk import Rake\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from gensim.test.utils import datapath\n",
    "import math\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Tree to JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please run this code cell once only as multiple runs will affect the same file \n",
    "with open('Clean_mind_map.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "itemsKeyedById = {i[\"ID\"]: i for i in data}\n",
    "\n",
    "#iterate through each item in the `myJson` list.\n",
    "for item in data[1:]:\n",
    "    #does the item have a parent?\n",
    "    if \"Parent\" in item:\n",
    "        #get the parent item\n",
    "        parent = itemsKeyedById[item['Parent']]\n",
    "        #if the parent item doesn't have a \"children\" member, \n",
    "        #we must create one.\n",
    "        if \"children\" not in parent:\n",
    "            parent[\"children\"] = []\n",
    "        #add the item to its parent's \"children\" list.\n",
    "        parent[\"children\"].append(item)\n",
    "topLevelItems = [item for item in data if \"Parent\" not in item]\n",
    "taxonomy_data = {'data': data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(taxonomy_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples are found on this link: http://jmespath.org/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression = jmespath.compile(\"data[?Title =='Lean of Peak'].children[].Title\")\n",
    "expression.search(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LDA_vector.csv',\n",
       " 'COPA_forum_threads.csv',\n",
       " 'COPA_forum_replies.csv',\n",
       " 'COPA_Wiki.csv',\n",
       " 'COPA_blog.csv',\n",
       " 'COPASearches.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for filename in glob.glob('*.csv'):\n",
    "    data.append(filename)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>WikiId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Title</th>\n",
       "      <th>FormattedBody</th>\n",
       "      <th>LastModifiedUtcDate</th>\n",
       "      <th>ContentType</th>\n",
       "      <th>ContentId</th>\n",
       "      <th>TotalViews</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>759</td>\n",
       "      <td>6</td>\n",
       "      <td>2256</td>\n",
       "      <td>Cirrus accident discussions</td>\n",
       "      <td>asdf\\r\\n\\r\\n\\r\\n\\r\\nAccident\\r\\nDate\\r\\nLocati...</td>\n",
       "      <td>2008-12-21 20:06:53.923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>361</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>698</td>\n",
       "      <td>6</td>\n",
       "      <td>2256</td>\n",
       "      <td>Cirrus Fatal Accident #01 near Sierra Vista, AZ</td>\n",
       "      <td>\\r\\n\\r\\n\\r\\nCirrus Fatal Accident #1Sierra Vis...</td>\n",
       "      <td>2008-12-21 18:45:28.610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>740</td>\n",
       "      <td>6</td>\n",
       "      <td>2256</td>\n",
       "      <td>First page</td>\n",
       "      <td>http://www.cirruspilots.org/wikis/sandbox/add....</td>\n",
       "      <td>2008-12-21 11:49:32.547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>294</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>655</td>\n",
       "      <td>6</td>\n",
       "      <td>2256</td>\n",
       "      <td>Test infobox for accidents</td>\n",
       "      <td>\\r\\n\\r\\n\\r\\nCirrus Fatal Accident #40Rock Spri...</td>\n",
       "      <td>2008-12-21 15:21:53.423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>624</td>\n",
       "      <td>6</td>\n",
       "      <td>2256</td>\n",
       "      <td>Cirrus Fatal Accident #33 near Paso Robles, CA</td>\n",
       "      <td>\\r\\n\\r\\n\\r\\nCirrus Fatal Accident #33Paso Robl...</td>\n",
       "      <td>2008-12-21 18:30:41.250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>318</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  WikiId  UserId  \\\n",
       "0         759       6    2256   \n",
       "1         698       6    2256   \n",
       "2         740       6    2256   \n",
       "3         655       6    2256   \n",
       "4         624       6    2256   \n",
       "\n",
       "                                             Title  \\\n",
       "0                      Cirrus accident discussions   \n",
       "1  Cirrus Fatal Accident #01 near Sierra Vista, AZ   \n",
       "2                                       First page   \n",
       "3                       Test infobox for accidents   \n",
       "4   Cirrus Fatal Accident #33 near Paso Robles, CA   \n",
       "\n",
       "                                       FormattedBody      LastModifiedUtcDate  \\\n",
       "0  asdf\\r\\n\\r\\n\\r\\n\\r\\nAccident\\r\\nDate\\r\\nLocati...  2008-12-21 20:06:53.923   \n",
       "1  \\r\\n\\r\\n\\r\\nCirrus Fatal Accident #1Sierra Vis...  2008-12-21 18:45:28.610   \n",
       "2  http://www.cirruspilots.org/wikis/sandbox/add....  2008-12-21 11:49:32.547   \n",
       "3  \\r\\n\\r\\n\\r\\nCirrus Fatal Accident #40Rock Spri...  2008-12-21 15:21:53.423   \n",
       "4  \\r\\n\\r\\n\\r\\nCirrus Fatal Accident #33Paso Robl...  2008-12-21 18:30:41.250   \n",
       "\n",
       "  ContentType  ContentId  TotalViews  Rank  \n",
       "0         NaN        NaN         361     5  \n",
       "1         NaN        NaN         169     5  \n",
       "2         NaN        NaN         294     5  \n",
       "3         NaN        NaN          57     5  \n",
       "4         NaN        NaN         318     5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = pd.read_csv(data[3])\n",
    "wiki = wiki[['LastModifiedUtcDate', 'Title', 'FormattedBody', 'WikiId','UserId', 'TotalViews']]\n",
    "# wiki['Author'] = [a.strip() for a in wiki['Author']]\n",
    "# wiki['Comment'] = 0\n",
    "# wiki['Resource'] = 'Wiki'\n",
    "wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magazine = pd.read_csv(data[])\n",
    "# magazine_date_list = list(magazine['Magazine'].unique())\n",
    "# # Get the month of magazine from its name\n",
    "# # Since the the COPA magazines are released monthly, so it only has year and month\n",
    "# # in its date column. In this case we just assume all magazines were released on the \n",
    "# # first day of that month.\n",
    "# magazine_date_dict = {magazine_date_list[0]: '2015-01-01', magazine_date_list[1]: '2019-01-01',\n",
    "#                       magazine_date_list[2]: '2015-07-01', magazine_date_list[3]: '2019-04-01',\n",
    "#                       magazine_date_list[4]: '2019-06-01', magazine_date_list[5]: '2018-01-01',\n",
    "#                       magazine_date_list[6]: '2017-09-01', magazine_date_list[7]: '2016-09-01',\n",
    "#                       magazine_date_list[8]: '2017-11-01', magazine_date_list[9]: '2015-05-01',\n",
    "#                       magazine_date_list[10]: '2015-09-01', magazine_date_list[11]: '2006-09-01', \n",
    "#                       magazine_date_list[12]: '2018-06-01', magazine_date_list[13]: '2019-07-01',\n",
    "#                       magazine_date_list[14]: '2017-04-01', magazine_date_list[15]: '2016-04-01',\n",
    "#                       magazine_date_list[16]: '2018-11-01', magazine_date_list[17]: '2015-09-01',\n",
    "#                       magazine_date_list[18]: '2019-03-01', magazine_date_list[19]: '2006-11-01',\n",
    "#                       magazine_date_list[20]: '2016-01-01', magazine_date_list[21]: '2019-05-01',\n",
    "#                       magazine_date_list[22]: '2016-03-01', magazine_date_list[23]: '2016-05-01',\n",
    "#                       magazine_date_list[24]: '2006-07-01', magazine_date_list[25]: '2017-06-01',\n",
    "#                       magazine_date_list[26]: '2018-07-01', magazine_date_list[27]: '2018-09-01',\n",
    "#                       magazine_date_list[28]: '2017-01-01', magazine_date_list[29]: '2012-11-01'}\n",
    "# magazine['Magazine'] = magazine['Magazine'].map(magazine_date_dict)\n",
    "# magazine.columns = ['Date', 'Title', 'Content', 'Author']\n",
    "# magazine['Like'] = 0\n",
    "# magazine['Comment'] = 0\n",
    "# magazine['Resource'] = 'Magazine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Title</th>\n",
       "      <th>FormattedBody</th>\n",
       "      <th>TotalViews</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-09-16</td>\n",
       "      <td>2438</td>\n",
       "      <td>holds</td>\n",
       "      <td>\\r\\nDuring advanced instruction and in particu...</td>\n",
       "      <td>1345</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-12-26</td>\n",
       "      <td>7912</td>\n",
       "      <td>sunday night in anguilla</td>\n",
       "      <td>Well, I survived Christmas Eve and Christmas. ...</td>\n",
       "      <td>856</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-12-26</td>\n",
       "      <td>7912</td>\n",
       "      <td>sunday night in anguilla</td>\n",
       "      <td>Well, I survived Christmas Eve and Christmas. ...</td>\n",
       "      <td>856</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-10-19</td>\n",
       "      <td>2438</td>\n",
       "      <td>do you know how to do this a top 5 list of ite...</td>\n",
       "      <td>\\r\\nAs an ongoing series of blog posts revolvi...</td>\n",
       "      <td>1747</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-02-12</td>\n",
       "      <td>4736</td>\n",
       "      <td>copa sun and fun lunch april 4 2014 lake land fl</td>\n",
       "      <td>Rich Hayes, Governor of COPA South East invite...</td>\n",
       "      <td>285</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  UserID                                              Title  \\\n",
       "0  2008-09-16    2438                                              holds   \n",
       "1  2010-12-26    7912                           sunday night in anguilla   \n",
       "2  2010-12-26    7912                           sunday night in anguilla   \n",
       "3  2010-10-19    2438  do you know how to do this a top 5 list of ite...   \n",
       "4  2014-02-12    4736   copa sun and fun lunch april 4 2014 lake land fl   \n",
       "\n",
       "                                       FormattedBody  TotalViews Source  \n",
       "0  \\r\\nDuring advanced instruction and in particu...        1345   Blog  \n",
       "1  Well, I survived Christmas Eve and Christmas. ...         856   Blog  \n",
       "2  Well, I survived Christmas Eve and Christmas. ...         856   Blog  \n",
       "3  \\r\\nAs an ongoing series of blog posts revolvi...        1747   Blog  \n",
       "4  Rich Hayes, Governor of COPA South East invite...         285   Blog  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog = pd.read_csv(data[4])\n",
    "blog = blog[['PostDate', 'UserId', 'PostName', 'PostBody', 'TotalViews']]\n",
    "blog.columns = ['Date', 'UserID', 'Title', 'FormattedBody', 'TotalViews']\n",
    "blog['Source'] = 'Blog'\n",
    "blog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>UserID</th>\n",
       "      <th>FormattedBody</th>\n",
       "      <th>ThreadId</th>\n",
       "      <th>UserPoints</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-06-25</td>\n",
       "      <td>3870</td>\n",
       "      <td>You are cordially invited to a Cirrus BBQ &amp; Fu...</td>\n",
       "      <td>28054</td>\n",
       "      <td>32</td>\n",
       "      <td>Forum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-10-23</td>\n",
       "      <td>4255</td>\n",
       "      <td>Ed,Thank you for the offer. My envelope is on ...</td>\n",
       "      <td>16</td>\n",
       "      <td>239</td>\n",
       "      <td>Forum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-10-23</td>\n",
       "      <td>3426</td>\n",
       "      <td>If I come over Tuesday do I get to play Igor t...</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>Forum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-10-24</td>\n",
       "      <td>2296</td>\n",
       "      <td>Ed, My envelope is on the way also! thanks for...</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>Forum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-11-04</td>\n",
       "      <td>3586</td>\n",
       "      <td>When I returned home from S FL yesterday, I co...</td>\n",
       "      <td>16</td>\n",
       "      <td>182</td>\n",
       "      <td>Forum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  UserID                                      FormattedBody  \\\n",
       "0  2008-06-25    3870  You are cordially invited to a Cirrus BBQ & Fu...   \n",
       "1  2005-10-23    4255  Ed,Thank you for the offer. My envelope is on ...   \n",
       "2  2005-10-23    3426  If I come over Tuesday do I get to play Igor t...   \n",
       "3  2005-10-24    2296  Ed, My envelope is on the way also! thanks for...   \n",
       "4  2005-11-04    3586  When I returned home from S FL yesterday, I co...   \n",
       "\n",
       "   ThreadId  UserPoints Source  \n",
       "0     28054          32  Forum  \n",
       "1        16         239  Forum  \n",
       "2        16          35  Forum  \n",
       "3        16          17  Forum  \n",
       "4        16         182  Forum  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum = pd.read_csv(data[2])\n",
    "forum = forum[['ThreadReplyDate', 'UserID', 'FormattedBody', 'ThreadId', 'UserPoints']]\n",
    "forum.columns = ['Date', 'UserID', 'FormattedBody', 'ThreadId', 'UserPoints']\n",
    "forum['Date'] = forum['Date'].apply(lambda x: x.split(' ')[0])\n",
    "forum['Source'] = 'Forum'\n",
    "forum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Title</th>\n",
       "      <th>FormattedBody</th>\n",
       "      <th>TotalViews</th>\n",
       "      <th>Source</th>\n",
       "      <th>ThreadId</th>\n",
       "      <th>UserPoints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-09-16</td>\n",
       "      <td>2438</td>\n",
       "      <td>holds</td>\n",
       "      <td>\\r\\nDuring advanced instruction and in particu...</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>Blog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-12-26</td>\n",
       "      <td>7912</td>\n",
       "      <td>sunday night in anguilla</td>\n",
       "      <td>Well, I survived Christmas Eve and Christmas. ...</td>\n",
       "      <td>856.0</td>\n",
       "      <td>Blog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-12-26</td>\n",
       "      <td>7912</td>\n",
       "      <td>sunday night in anguilla</td>\n",
       "      <td>Well, I survived Christmas Eve and Christmas. ...</td>\n",
       "      <td>856.0</td>\n",
       "      <td>Blog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-10-19</td>\n",
       "      <td>2438</td>\n",
       "      <td>do you know how to do this a top 5 list of ite...</td>\n",
       "      <td>\\r\\nAs an ongoing series of blog posts revolvi...</td>\n",
       "      <td>1747.0</td>\n",
       "      <td>Blog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-02-12</td>\n",
       "      <td>4736</td>\n",
       "      <td>copa sun and fun lunch april 4 2014 lake land fl</td>\n",
       "      <td>Rich Hayes, Governor of COPA South East invite...</td>\n",
       "      <td>285.0</td>\n",
       "      <td>Blog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  UserID                                              Title  \\\n",
       "0  2008-09-16    2438                                              holds   \n",
       "1  2010-12-26    7912                           sunday night in anguilla   \n",
       "2  2010-12-26    7912                           sunday night in anguilla   \n",
       "3  2010-10-19    2438  do you know how to do this a top 5 list of ite...   \n",
       "4  2014-02-12    4736   copa sun and fun lunch april 4 2014 lake land fl   \n",
       "\n",
       "                                       FormattedBody  TotalViews Source  \\\n",
       "0  \\r\\nDuring advanced instruction and in particu...      1345.0   Blog   \n",
       "1  Well, I survived Christmas Eve and Christmas. ...       856.0   Blog   \n",
       "2  Well, I survived Christmas Eve and Christmas. ...       856.0   Blog   \n",
       "3  \\r\\nAs an ongoing series of blog posts revolvi...      1747.0   Blog   \n",
       "4  Rich Hayes, Governor of COPA South East invite...       285.0   Blog   \n",
       "\n",
       "   ThreadId  UserPoints  \n",
       "0       NaN         NaN  \n",
       "1       NaN         NaN  \n",
       "2       NaN         NaN  \n",
       "3       NaN         NaN  \n",
       "4       NaN         NaN  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([blog, forum], ignore_index=True, sort=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Title</th>\n",
       "      <th>FormattedBody</th>\n",
       "      <th>TotalViews</th>\n",
       "      <th>Source</th>\n",
       "      <th>ThreadId</th>\n",
       "      <th>UserPoints</th>\n",
       "      <th>SourceScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-09-16</td>\n",
       "      <td>2438</td>\n",
       "      <td>holds</td>\n",
       "      <td>\\r\\nDuring advanced instruction and in particu...</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>Blog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-12-26</td>\n",
       "      <td>7912</td>\n",
       "      <td>sunday night in anguilla</td>\n",
       "      <td>Well, I survived Christmas Eve and Christmas. ...</td>\n",
       "      <td>856.0</td>\n",
       "      <td>Blog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-12-26</td>\n",
       "      <td>7912</td>\n",
       "      <td>sunday night in anguilla</td>\n",
       "      <td>Well, I survived Christmas Eve and Christmas. ...</td>\n",
       "      <td>856.0</td>\n",
       "      <td>Blog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-10-19</td>\n",
       "      <td>2438</td>\n",
       "      <td>do you know how to do this a top 5 list of ite...</td>\n",
       "      <td>\\r\\nAs an ongoing series of blog posts revolvi...</td>\n",
       "      <td>1747.0</td>\n",
       "      <td>Blog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-02-12</td>\n",
       "      <td>4736</td>\n",
       "      <td>copa sun and fun lunch april 4 2014 lake land fl</td>\n",
       "      <td>Rich Hayes, Governor of COPA South East invite...</td>\n",
       "      <td>285.0</td>\n",
       "      <td>Blog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  UserID                                              Title  \\\n",
       "0  2008-09-16    2438                                              holds   \n",
       "1  2010-12-26    7912                           sunday night in anguilla   \n",
       "2  2010-12-26    7912                           sunday night in anguilla   \n",
       "3  2010-10-19    2438  do you know how to do this a top 5 list of ite...   \n",
       "4  2014-02-12    4736   copa sun and fun lunch april 4 2014 lake land fl   \n",
       "\n",
       "                                       FormattedBody  TotalViews Source  \\\n",
       "0  \\r\\nDuring advanced instruction and in particu...      1345.0   Blog   \n",
       "1  Well, I survived Christmas Eve and Christmas. ...       856.0   Blog   \n",
       "2  Well, I survived Christmas Eve and Christmas. ...       856.0   Blog   \n",
       "3  \\r\\nAs an ongoing series of blog posts revolvi...      1747.0   Blog   \n",
       "4  Rich Hayes, Governor of COPA South East invite...       285.0   Blog   \n",
       "\n",
       "   ThreadId  UserPoints  SourceScore  \n",
       "0       NaN         NaN            1  \n",
       "1       NaN         NaN            1  \n",
       "2       NaN         NaN            1  \n",
       "3       NaN         NaN            1  \n",
       "4       NaN         NaN            1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ranking the source, blog is the most important source so have 3\n",
    "source_dict = {'Blog': 1, 'Forum': 2}\n",
    "df['SourceScore'] = df['Source'].map(source_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate Recency Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  Calculate_RecencyScore(date):\n",
    "    '''\n",
    "    Recency Rate = log( 1 + 1/(days between the post date and current date + 1))\n",
    "    '''\n",
    "    date_datetime = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "    rececncy_rate = math.log10(1/(((datetime.date(datetime.now()))-date_datetime).days+1)+1)\n",
    "    return rececncy_rate\n",
    "    \n",
    "\n",
    "df['RecencyRate'] = df['Date'].map(lambda y: Calculate_RecencyScore(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate Author Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = list(df['UserID'].unique())\n",
    "author_score_dict = {}\n",
    "for author in author_list:\n",
    "    author_score = df[df['UserID'] == author]['SourceScore'].sum()\n",
    "    author_score_dict[author] = author_score\n",
    "    \n",
    "df['AuthorScore'] = df['UserID'].map(author_score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate Content Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'lda_model.pickle'\n",
    "infile = open(filename,'rb')\n",
    "lda_model = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'id2word.pickle'\n",
    "infile = open(filename,'rb')\n",
    "id2word = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = forum.FormattedBody.values.tolist()\n",
    "cleanlist = []\n",
    "for d in data:\n",
    "    try:\n",
    "        cleanlist.append(d.lower())\n",
    "    except:\n",
    "        continue        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put contents into model and assign each content to a topic\n",
    "texts1 = [[word for word in doc.split() if word not in stop_words] for doc in cleanlist[:100]]\n",
    "corpus1 = [id2word.doc2bow(text) for text in texts1]\n",
    "resultlist = lda_model.get_document_topics(corpus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary containing the index of content and its topic classification\n",
    "topic_dict = {}\n",
    "for i, result in enumerate(resultlist):\n",
    "    temp = 0\n",
    "    topic = 0\n",
    "    for j in range(len(result)):\n",
    "        if result[j][1] > temp:\n",
    "            temp = result[j][1]\n",
    "            topic = j\n",
    "    topic_dict[i] = topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare keyword vector with reference vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Match replies with searching terms by topic only__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_match(search_query):\n",
    "    '''\n",
    "    Extract the topic of searching query by LDA model trained by forum posts.\n",
    "    '''\n",
    "    texts1 = [[word for word in doc.lower().split() if word not in stop_words] for doc in [search_query]]\n",
    "    corpus1 = [id2word.doc2bow(t) for t in texts1]\n",
    "    result = lda_model.get_document_topics(corpus1)\n",
    "    LDA_topic = max(result[0], key=lambda x: x[1])[0]\n",
    "    return LDA_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_vector = topic_match('Cirrus Parachute First Responders')\n",
    "topic_match = [key for key, value in topic_dict.items() if value == reference_vector]\n",
    "reply_match_search = forum.iloc[topic_match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,\n",
       " '0.100*\"pilot\" + 0.034*\"fly\" + 0.030*\"training\" + 0.024*\"accident\" + 0.022*\"cap\" + 0.021*\"airplane\" + 0.020*\"cirrus\" + 0.017*\"flight\" + 0.017*\"plane\" + 0.014*\"safety\"')"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_vector, lda_model.print_topics(num_words = 10)[reference_vector][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In reply to:Even if CO doesn't turn out to be the cause I am beating myself up mercilessly over NOT having CO detection in my cockpit. What is the best one out there?I have from Sporty's the silver colored industrial electronic CO2 detector that sells for $295. (They have several listed in their catalog).  When I took delivery of my SR 20 three years ago, serial # 1604, while taking the factory transition training the CO2 detector went off when it was in a flight bag in the luggage compartment.  It turned out that the exhaust manifold was damaged.  There was no odor in the cockpit - the CO2 detector triggered during a long cross country flight.  The factory was surprised the detector went off.  The factory ended up replacing the exhaust manifold and there has been no problem since.  I currently put the C02 detector in the little \"glove box\" hole above the heating controls of my SR 20 where I also keep the checklist, a pad, and pencils.SR20 Cirrus N356JP, Serial # 1604 \n",
      "\n",
      "Same battery your BIOS runs on with yr computer motherboard. \n",
      "\n",
      "In reply to:MikeMaybe not carbon MONoxide (CO), but wouldn't the carbon DIoxide (CO2) be a problem, and cause death in sufficient quantities?I think only to the extent that it displaced oxygen.\n",
      "\n",
      "Thanks for the update. Do you recall the total time on those mags? Not since OH, but since new so we have a sense of the age of the nylon gears. Thanks in advance. \n",
      "\n",
      "Jeff,I have a separate XM antenna. I thought you could only get the dual antenna from Cirrus or Garmin. Does the SB permit a 3rd party purchase or are you referring to the Garmin STC?Thanks for the info.\n",
      "\n",
      "In reply to: Sounds like she (yes, A&P is a she and I'm very impressed so far) may be mistaken based on the cost for a new one you were quoted.Richard:If \"she\" is Robin at Encore Aviation, KFDK, I was very impressed by her as well.  I was on my east coast journey when my problem hit and took the plane to Frederick.    Robin and her team (including Dave, the head avionics guy) were first rate.    As for her being off on the numbers, if that's the case, prior to her present job, Robin ran the parts department for a while.   While I was on a conference call with some of the A&Ps she came in and said \"My goodness, I just saw what Cirrus wants for a new alternator!!\"    As I said, they did a good job.I'm not competent to say whether 500 hours is right for the alternator repair or not.   However, Cirrus did have problems with the shear couplers in alternators of our vintage.   Mine went out at about 300 hours.   It didn't require an alternator replacement and I understand that since then Cirrus has used a different coupler and not had a recurrence.    I think I've got that right (Mike Busch or someone else would know), but you might ask about it.Dan\n",
      "\n",
      "In reply to:And who cares - we're all PIC it's the passengers in the back seats that toss their cookies when the wagging starts :)Which is what happened a number of years ago when I took the first passenger in my A36 Bonanza with a BRAND NEW interior!I was screaming at him to just throw up in one place, not all over the whole damn passenger compartment! He had a barf bag in his hand the whole time and didn't get a single chunk in it.\n",
      "\n",
      "UPDATED list as of June 25th for the Catalina BBQ fly-in this Saturday, June 28th at 5 pm.Check out the menu here...BBQ tri-tip and a live band!http://www.catalinadc3.com/barbecues.htmlRSVP listRob and Denise Wilson (plus 1)Stuart SmithJack Gershfeld (plus 1?)Rik A. Vanooteghem (plus 1?)Phillip Friedman J. Robert MossJerry Brown (plus 3)Mike Finnell (plus ?)Please send PM or respond here if you plan to attend.\n",
      "\n",
      "Put a K&N air filter on a 2004 G2 at my last annual.  It works well.  I average 2k faster on most trips.  It is a pain to install though.  We had to make several calls to the manufacturer to get the right part (and paperwork).  It cost 1.4 hours of labor ($118) at the local SC.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = reply_match_search['FormattedBody'].reset_index(drop = True)\n",
    "for i in range(9):\n",
    "    print (result[i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Match replies with searching terms by comparing keywords vector and reference vector__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile(r'([.0-9]+)\\*\\\"([a-zA-Z]+)\\\"')\n",
    "keyword_vector = {}\n",
    "for i in range(20):\n",
    "    keyword_vector[i] = {}\n",
    "    word_weight = r.findall(lda_model.print_topics(num_words = 10)[i][1])\n",
    "    for weight, word in word_weight:\n",
    "        keyword_vector[i][word] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare query to each leaf node, if the partial match score is higher than the threshold, then return a combination vector\n",
    "# that contains all words in those leaf nodes.\n",
    "def fuzzywuzzy_match(query,threshold):\n",
    "    \n",
    "    # Generate a list contains all leaf nodes, each sublist contains all words in that leaf node.\n",
    "    leaf_node = list()\n",
    "    for item in topic_vector:\n",
    "        if len(topic_vector[item]) == 1:\n",
    "            leaf_node.append(topic_vector[item][0].split())\n",
    "            \n",
    "    reference_vector = list()\n",
    "    \n",
    "    for node in leaf_node:\n",
    "        query_len = len(query)\n",
    "        node_len = len(node)\n",
    "        \n",
    "        if query_len >= node_len: # If query is longer\n",
    "            for i in range(node_len):\n",
    "                score = fuzz.partial_ratio(node[i],query)\n",
    "                if score > threshold:\n",
    "                    reference_vector.append(node) \n",
    "                    \n",
    "        else: # If leaf node is longer\n",
    "            for j in range(query_len):\n",
    "                score = fuzz.partial_ratio(query[j],node)\n",
    "                if score > threshold:\n",
    "                    reference_vector.append(node)\n",
    "    \n",
    "    reference_vector = list(list(i) for i in set(map(tuple, reference_vector))) # Deduplication\n",
    "    \n",
    "    return reference_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 80\n",
    "query = \"Cirrus Parachute First Responders\"\n",
    "reference_vector = fuzzywuzzy_match(query,threshold)\n",
    "reference_vector_sim = []\n",
    "for vectors in reference_vector:\n",
    "    for words in vectors:\n",
    "        reference_vector_sim.append(words.lower())\n",
    "        \n",
    "reference_vector_sim = list(set(reference_vector_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0\n",
    "for i in range(19):\n",
    "    match = set(keyword_vector[i].keys()) & set(reference_vector_sim)\n",
    "    if len(match) >= 1:\n",
    "        weight = keyword_vector[i][list(match)[0]]\n",
    "        if float(weight) >= float(temp):\n",
    "            temp = weight\n",
    "            relevant_topic = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_match = [key for key, value in topic_dict.items() if value == relevant_topic]\n",
    "reply_match_search = forum.iloc[topic_match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '0.026*\"system\" + 0.022*\"work\" + 0.022*\"update\" + 0.019*\"use\" + 0.018*\"mfd\" + 0.018*\"datum\" + 0.018*\"display\" + 0.017*\"software\" + 0.014*\"avidyne\" + 0.013*\"avionic\"')"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_topic, lda_model.print_topics(num_words = 10)[relevant_topic][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If I come over Tuesday do I get to play Igor the lowly apprentice? Since your so conservative, I'll make the one's for the Left sideTrip\n",
      "\n",
      "In reply to:Usual disclaimers:For display use only!No express or implied warranty of fitness for purpose!May or may not work!Use at you own risk!Do not ingest - for external use only!etc. etc. (don't say you weren't warned!)Fast Eddie:I enjoyed your legal disclaimers.  Last night while watching the LA Clippers play Phoenix I LMAO when I saw the legal disclaimer at the bottom of this ad Click Here.I hope Dennis can give us some legal expertise on the disclaimer in this ad.[;)]\n",
      "\n",
      "Set going out today to Mike Danko.BTW, the reason I post this here is:1) to bump the thread to the top for newbies, and,2) to keep a record of who I've sent them to and whenEnjoy!\n",
      "\n",
      "Fast Eddie,I made two of these up when you first introduced them. That was quite a while ago. Home here in Vermont, I have the plane hangered, But when I go to Florida I use them. I also use them when I wash the plane. They work GREAT.\n",
      "\n",
      "Going out in today's mail:Donald Thompson, Jr.Keith LambDave BushmanClarke Broadcasting?Enjoy!\n",
      "\n",
      "Sets going out today to:Andrew BarnardGeorge Griffithand some attorney in Miami [;)].Remember, Dennis, they're for display use only!\n",
      "\n",
      "Sets going out in today's mail to:Lars HelgesenTom TillmanFor those who sent \"contributions\", much thanks. It's not required, or even requested, but it does help me cover the modest costs involved.Fly safe!\n",
      "\n",
      "Know you have many responses but would gladly purchase a set of covers for my SR22.  Never posted before - if able thanks! if not thanks!Strate960 CR 4779Sulphur Springs, TX 75482\n",
      "\n",
      "Thanks Ed, I'm sending you an envelope.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = reply_match_search['FormattedBody'].reset_index(drop = True)\n",
    "for i in range(9):\n",
    "    print (result[i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk.\n",
    "temp_file = datapath(\"lda.model\")\n",
    "lda_model.save(temp_file)\n",
    "\n",
    "# Load lda.model\n",
    "lda = gensim.models.ldamodel.LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(lda_model.print_topics(num_words = 10), columns = ['Topic', 'Keywords']).to_csv('LDA_vector.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Normalization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalization(col):\n",
    "    nomolized_col = (df[col] - df[col].mean()) / df.loc[:, col].std()\n",
    "    return nomolized_col\n",
    "\n",
    "df['RecencyRate'] = Normalization('RecencyRate')\n",
    "df['AuthorScore'] = Normalization('AuthorScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Author</th>\n",
       "      <th>Like</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Resource</th>\n",
       "      <th>SourceScore</th>\n",
       "      <th>RecencyRate</th>\n",
       "      <th>AuthorScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>President's Column</td>\n",
       "      <td>JANUARY FEBRUARY 20154CIRRUS PILOTAs this is b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Magazine</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.465959</td>\n",
       "      <td>-1.209951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>COPA News</td>\n",
       "      <td>JANUARY FEBRUARY 20156CIRRUS PILOTWith this is...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Magazine</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.465959</td>\n",
       "      <td>-1.209951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Regional News</td>\n",
       "      <td>JANUARY FEBRUARY 201512CIRRUS PILOTby GIL WILL...</td>\n",
       "      <td>GIL WILLIAMSON</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Magazine</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.465959</td>\n",
       "      <td>0.211050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Cirrus Perspective</td>\n",
       "      <td>JANUARY FEBRUARY 201518CIRRUS PILOTIts hard to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Magazine</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.465959</td>\n",
       "      <td>-1.209951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Member Spotlight</td>\n",
       "      <td>JANUARY FEBRUARY 201522CIRRUS PILOTCirrus Pilo...</td>\n",
       "      <td>KIM BLONIGEN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Magazine</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.465959</td>\n",
       "      <td>0.211050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date               Title  \\\n",
       "0  2015-01-01  President's Column   \n",
       "1  2015-01-01           COPA News   \n",
       "2  2015-01-01       Regional News   \n",
       "3  2015-01-01  Cirrus Perspective   \n",
       "4  2015-01-01    Member Spotlight   \n",
       "\n",
       "                                             Content          Author  Like  \\\n",
       "0  JANUARY FEBRUARY 20154CIRRUS PILOTAs this is b...             NaN     0   \n",
       "1  JANUARY FEBRUARY 20156CIRRUS PILOTWith this is...             NaN     0   \n",
       "2  JANUARY FEBRUARY 201512CIRRUS PILOTby GIL WILL...  GIL WILLIAMSON     0   \n",
       "3  JANUARY FEBRUARY 201518CIRRUS PILOTIts hard to...             NaN     0   \n",
       "4  JANUARY FEBRUARY 201522CIRRUS PILOTCirrus Pilo...    KIM BLONIGEN     0   \n",
       "\n",
       "   Comment  Resource  SourceScore  RecencyRate  AuthorScore  \n",
       "0        0  Magazine            3    -0.465959    -1.209951  \n",
       "1        0  Magazine            3    -0.465959    -1.209951  \n",
       "2        0  Magazine            3    -0.465959     0.211050  \n",
       "3        0  Magazine            3    -0.465959    -1.209951  \n",
       "4        0  Magazine            3    -0.465959     0.211050  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input Searching query__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Search Results Ranking\n",
    "\n",
    "- \"Cirrus Parachute First Responders\"\n",
    "- \"Different between AATD and FTD\"\n",
    "- \"Activate vector to final\"\n",
    "- \"Turbo normalize vs turbo charge\"\n",
    "- \"Shooting ILS approach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(search_query):\n",
    "    '''\n",
    "    Get the most important keyword from searching query \n",
    "    '''\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.append('vs')\n",
    "    r = Rake(stopwords = stop_words)\n",
    "    \n",
    "    r.extract_keywords_from_text(search_query)\n",
    "    rank = r.get_ranked_phrases_with_scores()\n",
    "    itemMaxValue = max(rank, key=lambda x: x[0])[0]\n",
    "    \n",
    "    # Find the most important keyword\n",
    "    listOfKeys = list()\n",
    "    for value, key in rank:\n",
    "        if value == itemMaxValue:\n",
    "            listOfKeys.append(key.lower())\n",
    "    \n",
    "    return listOfKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_query = [\"Cirrus Parachute First Responders\", \"Difference between AATD and FTD\", \"Activate vector to final\", \"Turbo normalize vs turbo charge\", \"Shooting ILS approach\"]\n",
    "# extract_keywords(\"Difference between AATD and FTD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_terms = [tree['Title'] for tree in taxonomy_data['data'][1:]]\n",
    "\n",
    "topic_vector = {}\n",
    "for i in range(len(taxonomy_terms)):\n",
    "    topic_info = taxonomy_data['data'][1:][i] # Sub level of all terms other than COPA\n",
    "    title = re.findall(r'Title\\': \\'(.*?)\\'', str(topic_info)) # Extract all children titles under specific term\n",
    "    if len(title) > 1: # If the term is among the bottom level, then just assign itself to the term\n",
    "        topic_vector[taxonomy_terms[i].lower()] = title[1:]\n",
    "    else:\n",
    "        topic_vector[taxonomy_terms[i].lower()] = [taxonomy_terms[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about which fuzzywuzzy method we should use:\n",
    "For each vector in topic_vector: which level we should match to.\n",
    "What kind of vector we want to get in the end? \n",
    "Current version: Combine all the match vectors together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 80  # Maybe adjusted to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare query to each leaf node, if the partial match score is higher than the threshold, then return a combination vector\n",
    "# that contains all words in those leaf nodes.\n",
    "def fuzzywuzzy_match(query,threshold):\n",
    "    \n",
    "    # Generate a list contains all leaf nodes, each sublist contains all words in that leaf node.\n",
    "    leaf_node = list()\n",
    "    for item in topic_vector:\n",
    "        if len(topic_vector[item]) == 1:\n",
    "            leaf_node.append(topic_vector[item][0].split())\n",
    "            \n",
    "    reference_vector = list()\n",
    "    \n",
    "    for node in leaf_node:\n",
    "        query_len = len(query)\n",
    "        node_len = len(node)\n",
    "        \n",
    "        if query_len >= node_len: # If query is longer\n",
    "            for i in range(node_len):\n",
    "                score = fuzz.partial_ratio(node[i],query)\n",
    "                if score > threshold:\n",
    "                    reference_vector.append(node) \n",
    "                    \n",
    "        else: # If leaf node is longer\n",
    "            for j in range(query_len):\n",
    "                score = fuzz.partial_ratio(query[j],node)\n",
    "                if score > threshold:\n",
    "                    reference_vector.append(node)\n",
    "    \n",
    "    reference_vector = list(list(i) for i in set(map(tuple, reference_vector))) # Deduplication\n",
    "    \n",
    "    return reference_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Parachute'],\n",
       " ['Cirrus', 'Airframe', 'Parachute', 'System'],\n",
       " ['Cirrus', 'Approach'],\n",
       " ['Parachute', 'Disreef'],\n",
       " ['Over', 'a', 'Runaway'],\n",
       " ['Reefed', 'Parachute'],\n",
       " ['Rocket', 'and', 'Parachute', 'Extraction'],\n",
       " ['Not', 'Over', 'a', 'Runaway']]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Cirrus Parachute First Responders\"\n",
    "fuzzywuzzy_match(query,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(search_query):\n",
    "    '''\n",
    "    The searching query can be exact matched by taxonomy tree.\n",
    "    '''\n",
    "    if search_query in taxonomy_terms:\n",
    "        taxonomy_vector = topic_vector[search_query.lower()]\n",
    "    return taxonomy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Goodyear', 'Michelin', 'Aero Classic', 'STA']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exact_match('Tire Brands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector that stores all the phrases(more than one word) that should be \n",
    "# token as a whole.\n",
    "def generate_phrases_tokenizer(topic_vector): # Return a list of tuples containing phrases.\n",
    "    all_multiword_tokens = list()\n",
    "    for item in topic_vector: \n",
    "        for phrase in topic_vector[item]:\n",
    "            phrase = phrase.replace('(','') # Eliminate '(' and ')' in the phrase\n",
    "            phrase = phrase.replace(')','')\n",
    "           #phrase = re.sub(r'( \\(.*\\))','',phrase) #exclude all contents in '()'\n",
    "            phrase_lower = phrase.lower() # Convert all characters into lower case.\n",
    "            #phrase_lower.replace('-', ' ')\n",
    "            word_lst = phrase_lower.split()\n",
    "            if len(word_lst) >= 2: \n",
    "                word_tuple = tuple(word_lst)\n",
    "                all_multiword_tokens.append(word_tuple)  \n",
    "    all_multiword_tokens = list(set(all_multiword_tokens)) # Deduplication\n",
    "    tokenizer = MWETokenizer(all_multiword_tokens,separator=' ')\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_match(search_query,tokenizer):\n",
    "    '''\n",
    "    Any words in searching query can be exact matched by taxonomy tree.\n",
    "    '''\n",
    "    # Split input string into words or phrases according to taxonomy tree.\n",
    "    words = tokenizer.tokenize(search_query.split())\n",
    "    taxonomy_vector = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            taxonomy_vector.append(topic_vector[word])\n",
    "        except:\n",
    "            continue\n",
    "    return taxonomy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Turbo']]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_tokenizer = generate_phrases_tokenizer(topic_vector)\n",
    "any_match(\"Turbo normalize vs turbo charge\", phrase_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_match(keywords):\n",
    "    '''\n",
    "    The most important keyword of searching query can be matched by taxonomy tree.\n",
    "    '''\n",
    "    taxonomy_vector = []\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            taxonomy_vector.append(topic_vector[keyword])\n",
    "        except:\n",
    "            continue\n",
    "    return taxonomy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Engine']], ['work', 'engine'])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keywords = extract_keywords(\"Engine doesn't work.\")\n",
    "# keyword_match(keywords), keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_match(search_query):\n",
    "    '''\n",
    "    Extract the topic of searching query by LDA model trained by forum posts.\n",
    "    '''\n",
    "    texts1 = [[word for word in doc.lower().split() if word not in stop_words] for doc in [search_query]]\n",
    "    corpus1 = [id2word.doc2bow(t) for t in texts1]\n",
    "    result = lda_model.get_document_topics(corpus1)\n",
    "    LDA_topic = max(result[0], key=lambda x: x[1])[0]\n",
    "    return LDA_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,\n",
       " (12,\n",
       "  '0.101*\"altitude\" + 0.083*\"high\" + 0.065*\"climb\" + 0.041*\"low\" + 0.034*\"performance\" + 0.032*\"gun\" + 0.030*\"turbo\" + 0.028*\"level\" + 0.028*\"foot\" + 0.027*\"rate\"'))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topic_match('Turbo normalize vs turbo charge'), lda_model.print_topics(num_words = 10)[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
