{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Install packages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import statistics \n",
    "import jmespath\n",
    "import json\n",
    "from rake_nltk import Rake\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from gensim.test.utils import datapath\n",
    "import math\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Tree to JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please run this code cell once only as multiple runs will affect the same file \n",
    "with open('Clean_mind_map.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "itemsKeyedById = {i[\"ID\"]: i for i in data}\n",
    "\n",
    "#iterate through each item in the `myJson` list.\n",
    "for item in data[1:]:\n",
    "    #does the item have a parent?\n",
    "    if \"Parent\" in item:\n",
    "        #get the parent item\n",
    "        parent = itemsKeyedById[item['Parent']]\n",
    "        #if the parent item doesn't have a \"children\" member, \n",
    "        #we must create one.\n",
    "        if \"children\" not in parent:\n",
    "            parent[\"children\"] = []\n",
    "        #add the item to its parent's \"children\" list.\n",
    "        parent[\"children\"].append(item)\n",
    "topLevelItems = [item for item in data if \"Parent\" not in item]\n",
    "taxonomy_data = {'data': data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(taxonomy_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples are found on this link: http://jmespath.org/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression = jmespath.compile(\"data[?Title =='Lean of Peak'].children[].Title\")\n",
    "expression.search(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LDA_vector.csv',\n",
       " 'COPA_forum_threads.csv',\n",
       " 'COPA_forum_replies.csv',\n",
       " 'wiki_formal_total_clean.csv',\n",
       " 'COPA_Blogs.csv',\n",
       " 'COPASearches.csv']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for filename in glob.glob('*.csv'):\n",
    "    data.append(filename)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = pd.read_csv(data[3])\n",
    "wiki = wiki[['Date', 'Title', 'Content', 'Author','Like']]\n",
    "wiki['Author'] = [a.strip() for a in wiki['Author']]\n",
    "wiki['Comment'] = 0\n",
    "wiki['Resource'] = 'Wiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magazine = pd.read_csv(data[])\n",
    "# magazine_date_list = list(magazine['Magazine'].unique())\n",
    "# # Get the month of magazine from its name\n",
    "# # Since the the COPA magazines are released monthly, so it only has year and month\n",
    "# # in its date column. In this case we just assume all magazines were released on the \n",
    "# # first day of that month.\n",
    "# magazine_date_dict = {magazine_date_list[0]: '2015-01-01', magazine_date_list[1]: '2019-01-01',\n",
    "#                       magazine_date_list[2]: '2015-07-01', magazine_date_list[3]: '2019-04-01',\n",
    "#                       magazine_date_list[4]: '2019-06-01', magazine_date_list[5]: '2018-01-01',\n",
    "#                       magazine_date_list[6]: '2017-09-01', magazine_date_list[7]: '2016-09-01',\n",
    "#                       magazine_date_list[8]: '2017-11-01', magazine_date_list[9]: '2015-05-01',\n",
    "#                       magazine_date_list[10]: '2015-09-01', magazine_date_list[11]: '2006-09-01', \n",
    "#                       magazine_date_list[12]: '2018-06-01', magazine_date_list[13]: '2019-07-01',\n",
    "#                       magazine_date_list[14]: '2017-04-01', magazine_date_list[15]: '2016-04-01',\n",
    "#                       magazine_date_list[16]: '2018-11-01', magazine_date_list[17]: '2015-09-01',\n",
    "#                       magazine_date_list[18]: '2019-03-01', magazine_date_list[19]: '2006-11-01',\n",
    "#                       magazine_date_list[20]: '2016-01-01', magazine_date_list[21]: '2019-05-01',\n",
    "#                       magazine_date_list[22]: '2016-03-01', magazine_date_list[23]: '2016-05-01',\n",
    "#                       magazine_date_list[24]: '2006-07-01', magazine_date_list[25]: '2017-06-01',\n",
    "#                       magazine_date_list[26]: '2018-07-01', magazine_date_list[27]: '2018-09-01',\n",
    "#                       magazine_date_list[28]: '2017-01-01', magazine_date_list[29]: '2012-11-01'}\n",
    "# magazine['Magazine'] = magazine['Magazine'].map(magazine_date_dict)\n",
    "# magazine.columns = ['Date', 'Title', 'Content', 'Author']\n",
    "# magazine['Like'] = 0\n",
    "# magazine['Comment'] = 0\n",
    "# magazine['Resource'] = 'Magazine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog = pd.read_csv(data[4], encoding = 'unicode_escape')\n",
    "blog = blog[['Date', 'Title', 'Body', 'Author', 'Like','Comment']]\n",
    "blog.columns = ['Date', 'Title', 'Content', 'Author', 'Like','Comment']\n",
    "blog['Date'] = [d.split(' ')[0].split('/')[2] + '-' + d.split(' ')[0].split('/')[0] for d in blog['Date']]\n",
    "blog['Date'] = blog['Date'].map(lambda x: x + '-01')\n",
    "blog['Resource'] = 'Blog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ThreadReplyDate</th>\n",
       "      <th>UserID</th>\n",
       "      <th>FormattedBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-06-25 10:57:03</td>\n",
       "      <td>3870</td>\n",
       "      <td>You are cordially invited to a Cirrus BBQ &amp; Fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-10-23 00:56:48</td>\n",
       "      <td>4255</td>\n",
       "      <td>Ed,Thank you for the offer. My envelope is on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-10-23 02:02:14</td>\n",
       "      <td>3426</td>\n",
       "      <td>If I come over Tuesday do I get to play Igor t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-10-24 04:31:25</td>\n",
       "      <td>2296</td>\n",
       "      <td>Ed, My envelope is on the way also! thanks for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-11-04 01:34:09</td>\n",
       "      <td>3586</td>\n",
       "      <td>When I returned home from S FL yesterday, I co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ThreadReplyDate  UserID  \\\n",
       "0  2008-06-25 10:57:03    3870   \n",
       "1  2005-10-23 00:56:48    4255   \n",
       "2  2005-10-23 02:02:14    3426   \n",
       "3  2005-10-24 04:31:25    2296   \n",
       "4  2005-11-04 01:34:09    3586   \n",
       "\n",
       "                                       FormattedBody  \n",
       "0  You are cordially invited to a Cirrus BBQ & Fu...  \n",
       "1  Ed,Thank you for the offer. My envelope is on ...  \n",
       "2  If I come over Tuesday do I get to play Igor t...  \n",
       "3  Ed, My envelope is on the way also! thanks for...  \n",
       "4  When I returned home from S FL yesterday, I co...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum = pd.read_csv(data[2])\n",
    "forum = forum[['ThreadReplyDate', 'UserID', 'FormattedBody']]\n",
    "forum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdList = [wiki, blog]\n",
    "# Combine three datasets together\n",
    "df = pd.concat(pdList)\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ranking the source, blog is the most important source so have 3\n",
    "raw_data = {'Resource': ['Wiki','Blog','Magazine'], 'SourceScore': [1, 2, 3]}\n",
    "Source_tb = pd.DataFrame(raw_data, columns = ['Resource', 'SourceScore'])\n",
    "#add source score to the main table\n",
    "df = pd.merge(df, Source_tb, left_on = 'Resource', right_on = 'Resource')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate Recency Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  Calculate_RecencyScore(date):\n",
    "    '''\n",
    "    Recency Rate = log( 1 + 1/(days between the post date and current date + 1))\n",
    "    '''\n",
    "    date_datetime = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "    rececncy_rate = math.log10(1/(((datetime.date(datetime.now()))-date_datetime).days+1)+1)\n",
    "    return rececncy_rate\n",
    "    \n",
    "\n",
    "df['RecencyRate'] = df['Date'].map(lambda y: Calculate_RecencyScore(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate Author Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_AuthorScore(author):\n",
    "    '''\n",
    "    AuthorScore = Number of posts of specified content source * pre-determined weight\n",
    "    '''\n",
    "    author_score = df[df['Author'] == author]['SourceScore'].sum()\n",
    "    return author_score\n",
    "\n",
    "author_list = list(df['Author'].unique())\n",
    "df['AuthorScore'] = df['Author'].map(lambda x: Calculate_AuthorScore(x), author_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate Content Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917\n",
      "475\n",
      "3351\n",
      "[(0,\n",
      "  '0.026*\"system\" + 0.022*\"work\" + 0.022*\"update\" + 0.019*\"use\" + 0.018*\"mfd\" '\n",
      "  '+ 0.018*\"datum\" + 0.018*\"display\" + 0.017*\"software\" + 0.014*\"avidyne\" + '\n",
      "  '0.013*\"avionic\"'),\n",
      " (1,\n",
      "  '0.040*\"pay\" + 0.030*\"year\" + 0.020*\"cost\" + 0.019*\"company\" + '\n",
      "  '0.018*\"insurance\" + 0.016*\"business\" + 0.016*\"money\" + 0.015*\"claim\" + '\n",
      "  '0.014*\"tax\" + 0.013*\"rate\"'),\n",
      " (2,\n",
      "  '0.108*\"ice\" + 0.035*\"vote\" + 0.028*\"energy\" + 0.027*\"condition\" + '\n",
      "  '0.026*\"medical\" + 0.020*\"visual\" + 0.018*\"drug\" + 0.015*\"registration\" + '\n",
      "  '0.013*\"race\" + 0.013*\"image\"'),\n",
      " (3,\n",
      "  '0.033*\"turn\" + 0.024*\"mode\" + 0.023*\"approach\" + 0.022*\"hold\" + '\n",
      "  '0.020*\"course\" + 0.020*\"head\" + 0.019*\"switch\" + 0.018*\"fix\" + '\n",
      "  '0.018*\"autopilot\" + 0.016*\"button\"'),\n",
      " (4,\n",
      "  '0.073*\"land\" + 0.064*\"runway\" + 0.061*\"landing\" + 0.046*\"airport\" + '\n",
      "  '0.030*\"field\" + 0.028*\"pattern\" + 0.027*\"wind\" + 0.022*\"final\" + '\n",
      "  '0.018*\"turn\" + 0.016*\"approach\"'),\n",
      " (5,\n",
      "  '0.052*\"weight\" + 0.044*\"law\" + 0.043*\"carry\" + 0.036*\"woman\" + 0.034*\"food\" '\n",
      "  '+ 0.033*\"man\" + 0.033*\"eat\" + 0.023*\"legal\" + 0.022*\"pic\" + 0.021*\"city\"'),\n",
      " (6,\n",
      "  '0.094*\"engine\" + 0.068*\"fuel\" + 0.043*\"oil\" + 0.026*\"run\" + '\n",
      "  '0.019*\"cylinder\" + 0.018*\"pressure\" + 0.017*\"hour\" + 0.016*\"flow\" + '\n",
      "  '0.015*\"hot\" + 0.012*\"pump\"'),\n",
      " (7,\n",
      "  '0.094*\"thank\" + 0.030*\"call\" + 0.029*\"know\" + 0.021*\"com\" + 0.021*\"good\" + '\n",
      "  '0.020*\"great\" + 0.020*\"question\" + 0.019*\"send\" + 0.018*\"ask\" + '\n",
      "  '0.018*\"help\"'),\n",
      " (8,\n",
      "  '0.026*\"power\" + 0.019*\"altitude\" + 0.019*\"speed\" + 0.018*\"high\" + '\n",
      "  '0.013*\"climb\" + 0.013*\"would\" + 0.012*\"go\" + 0.012*\"low\" + 0.009*\"get\" + '\n",
      "  '0.009*\"trim\"'),\n",
      " (9,\n",
      "  '0.031*\"go\" + 0.028*\"get\" + 0.027*\"time\" + 0.025*\"fly\" + 0.023*\"year\" + '\n",
      "  '0.018*\"day\" + 0.017*\"take\" + 0.014*\"good\" + 0.012*\"back\" + 0.011*\"last\"'),\n",
      " (10,\n",
      "  '0.021*\"would\" + 0.019*\"say\" + 0.019*\"think\" + 0.016*\"make\" + 0.015*\"user\" + '\n",
      "  '0.014*\"know\" + 0.012*\"people\" + 0.009*\"thing\" + 0.009*\"go\" + 0.008*\"good\"'),\n",
      " (11,\n",
      "  '0.130*\"write\" + 0.130*\"follow\" + 0.106*\"user\" + 0.058*\"post\" + '\n",
      "  '0.028*\"quote\" + 0.022*\"copa\" + 0.017*\"thread\" + 0.016*\"member\" + '\n",
      "  '0.013*\"read\" + 0.013*\"may\"'),\n",
      " (12,\n",
      "  '0.035*\"fly\" + 0.034*\"flight\" + 0.021*\"approach\" + 0.019*\"weather\" + '\n",
      "  '0.017*\"would\" + 0.014*\"plan\" + 0.013*\"time\" + 0.012*\"get\" + 0.011*\"traffic\" '\n",
      "  '+ 0.010*\"go\"'),\n",
      " (13,\n",
      "  '0.100*\"pilot\" + 0.034*\"fly\" + 0.030*\"training\" + 0.024*\"accident\" + '\n",
      "  '0.022*\"cap\" + 0.021*\"airplane\" + 0.020*\"cirrus\" + 0.017*\"flight\" + '\n",
      "  '0.017*\"plane\" + 0.014*\"safety\"'),\n",
      " (14,\n",
      "  '0.022*\"wing\" + 0.022*\"side\" + 0.020*\"right\" + 0.018*\"water\" + 0.016*\"leave\" '\n",
      "  '+ 0.015*\"open\" + 0.012*\"door\" + 0.011*\"small\" + 0.010*\"front\" + '\n",
      "  '0.010*\"plane\"'),\n",
      " (15,\n",
      "  '0.032*\"aircraft\" + 0.027*\"ad\" + 0.020*\"require\" + 0.012*\"part\" + '\n",
      "  '0.012*\"traffic\" + 0.011*\"report\" + 0.010*\"provide\" + 0.009*\"system\" + '\n",
      "  '0.009*\"include\" + 0.009*\"requirement\"'),\n",
      " (16,\n",
      "  '0.109*\"look\" + 0.063*\"video\" + 0.053*\"see\" + 0.052*\"seat\" + 0.044*\"picture\" '\n",
      "  '+ 0.036*\"watch\" + 0.033*\"view\" + 0.028*\"photo\" + 0.027*\"show\" + '\n",
      "  '0.025*\"cool\"'),\n",
      " (17,\n",
      "  '0.030*\"plane\" + 0.025*\"would\" + 0.023*\"cirrus\" + 0.019*\"new\" + 0.017*\"buy\" '\n",
      "  '+ 0.013*\"think\" + 0.013*\"cost\" + 0.012*\"year\" + 0.011*\"much\" + '\n",
      "  '0.011*\"sell\"'),\n",
      " (18,\n",
      "  '0.035*\"car\" + 0.018*\"drive\" + 0.017*\"school\" + 0.016*\"paint\" + 0.013*\"old\" '\n",
      "  '+ 0.011*\"student\" + 0.011*\"plant\" + 0.010*\"advantage\" + 0.009*\"color\" + '\n",
      "  '0.009*\"kid\"'),\n",
      " (19,\n",
      "  '0.021*\"issue\" + 0.020*\"battery\" + 0.019*\"problem\" + 0.015*\"replace\" + '\n",
      "  '0.015*\"shop\" + 0.013*\"work\" + 0.012*\"check\" + 0.012*\"would\" + 0.010*\"time\" '\n",
      "  '+ 0.009*\"annual\"')]\n",
      "5358\n"
     ]
    }
   ],
   "source": [
    "t1 = datetime.now()\n",
    "\n",
    "forum = pd.read_csv(data[2])\n",
    "data = forum.FormattedBody.values.tolist()\n",
    "\n",
    "# Create a model for topic classification based on the dataset we have \n",
    "# (It may cause some problems because the model will be applied to the dataset later to give each content a topic.)\n",
    "# (The problem can be solved when we get the entire dataset since we can take part of the contents as corpus.)\n",
    "\n",
    "# Get some stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "t2 = datetime.now()\n",
    "diff = t2 - t1\n",
    "print (diff.seconds)\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "t3 = datetime.now()\n",
    "diff = t3 - t2\n",
    "print (diff.seconds)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "t4 = datetime.now()\n",
    "diff = t4 - t3\n",
    "print (diff.seconds)\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           passes=10)\n",
    "\n",
    "pprint(lda_model.print_topics(num_words = 10))\n",
    "# Get 20 related topics\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "t5 = datetime.now()\n",
    "diff = t5 - t4\n",
    "print (diff.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanlist = []\n",
    "for d in data:\n",
    "    try:\n",
    "        cleanlist.append(d.lower())\n",
    "    except:\n",
    "        continue        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put contents into model and assign each content to a topic\n",
    "texts1 = [[word for word in doc.split() if word not in stop_words] for doc in cleanlist]\n",
    "corpus1 = [id2word.doc2bow(text) for text in texts1]\n",
    "resultlist = lda_model.get_document_topics(corpus1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary containing the index of content and its topic classification\n",
    "topic_dict = {}\n",
    "for i, result in enumerate(resultlist):\n",
    "    temp = 0\n",
    "    topic = 0\n",
    "    for j in range(len(result)):\n",
    "        if result[j][1] > temp:\n",
    "            temp = result[j][1]\n",
    "            topic = j\n",
    "    topic_dict[i] = topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034411"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare keyword vector with reference vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Match replies with searching terms by topic only__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_match(search_query):\n",
    "    '''\n",
    "    Extract the topic of searching query by LDA model trained by forum posts.\n",
    "    '''\n",
    "    texts1 = [[word for word in doc.lower().split() if word not in stop_words] for doc in [search_query]]\n",
    "    corpus1 = [id2word.doc2bow(t) for t in texts1]\n",
    "    result = lda_model.get_document_topics(corpus1)\n",
    "    LDA_topic = max(result[0], key=lambda x: x[1])[0]\n",
    "    return LDA_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_vector = topic_match('Cirrus Parachute First Responders')\n",
    "topic_match = [key for key, value in topic_dict.items() if value == reference_vector]\n",
    "reply_match_search = forum.iloc[topic_match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,\n",
       " '0.100*\"pilot\" + 0.034*\"fly\" + 0.030*\"training\" + 0.024*\"accident\" + 0.022*\"cap\" + 0.021*\"airplane\" + 0.020*\"cirrus\" + 0.017*\"flight\" + 0.017*\"plane\" + 0.014*\"safety\"')"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_vector, lda_model.print_topics(num_words = 10)[reference_vector][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557     In reply to:Even if CO doesn't turn out to be ...\n",
      "753     Same battery your BIOS runs on with yr compute...\n",
      "775     In reply to:MikeMaybe not carbon MONoxide (CO)...\n",
      "881     Thanks for the update. Do you recall the total...\n",
      "1346    Jeff,I have a separate XM antenna. I thought y...\n",
      "1382    In reply to: Sounds like she (yes, A&P is a sh...\n",
      "2341    In reply to:And who cares - we're all PIC it's...\n",
      "2606    UPDATED list as of June 25th for the Catalina ...\n",
      "2820    Put a K&N air filter on a 2004 G2 at my last a...\n",
      "2915    Larry,My post was triggered by your King Air c...\n",
      "Name: FormattedBody, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (reply_match_search['FormattedBody'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Match replies with searching terms by comparing keywords vector and reference vector__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile(r'([.0-9]+)\\*\\\"([a-zA-Z]+)\\\"')\n",
    "keyword_vector = {}\n",
    "for i in range(20):\n",
    "    keyword_vector[i] = {}\n",
    "    word_weight = r.findall(lda_model.print_topics(num_words = 10)[i][1])\n",
    "    for weight, word in word_weight:\n",
    "        keyword_vector[i][word] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare query to each leaf node, if the partial match score is higher than the threshold, then return a combination vector\n",
    "# that contains all words in those leaf nodes.\n",
    "def fuzzywuzzy_match(query,threshold):\n",
    "    \n",
    "    # Generate a list contains all leaf nodes, each sublist contains all words in that leaf node.\n",
    "    leaf_node = list()\n",
    "    for item in topic_vector:\n",
    "        if len(topic_vector[item]) == 1:\n",
    "            leaf_node.append(topic_vector[item][0].split())\n",
    "            \n",
    "    reference_vector = list()\n",
    "    \n",
    "    for node in leaf_node:\n",
    "        query_len = len(query)\n",
    "        node_len = len(node)\n",
    "        \n",
    "        if query_len >= node_len: # If query is longer\n",
    "            for i in range(node_len):\n",
    "                score = fuzz.partial_ratio(node[i],query)\n",
    "                if score > threshold:\n",
    "                    reference_vector.append(node) \n",
    "                    \n",
    "        else: # If leaf node is longer\n",
    "            for j in range(query_len):\n",
    "                score = fuzz.partial_ratio(query[j],node)\n",
    "                if score > threshold:\n",
    "                    reference_vector.append(node)\n",
    "    \n",
    "    reference_vector = list(list(i) for i in set(map(tuple, reference_vector))) # Deduplication\n",
    "    \n",
    "    return reference_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 80\n",
    "query = \"Cirrus Parachute First Responders\"\n",
    "reference_vector = fuzzywuzzy_match(query,threshold)\n",
    "reference_vector_sim = []\n",
    "for vectors in reference_vector:\n",
    "    for words in vectors:\n",
    "        reference_vector_sim.append(words.lower())\n",
    "        \n",
    "reference_vector_sim = list(set(reference_vector_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0\n",
    "for i in range(19):\n",
    "    match = set(keyword_vector[i].keys()) & set(reference_vector_sim)\n",
    "    if len(match) >= 1:\n",
    "        weight = keyword_vector[i][list(match)[0]]\n",
    "        if float(weight) >= float(temp):\n",
    "            temp = weight\n",
    "            relevant_topic = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_match = [key for key, value in topic_dict.items() if value == relevant_topic]\n",
    "reply_match_search = forum.iloc[topic_match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '0.026*\"system\" + 0.022*\"work\" + 0.022*\"update\" + 0.019*\"use\" + 0.018*\"mfd\" + 0.018*\"datum\" + 0.018*\"display\" + 0.017*\"software\" + 0.014*\"avidyne\" + 0.013*\"avionic\"')"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_topic, lda_model.print_topics(num_words = 10)[relevant_topic][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2     If I come over Tuesday do I get to play Igor t...\n",
      "17    In reply to:Usual disclaimers:For display use ...\n",
      "22    Set going out today to Mike Danko.BTW, the rea...\n",
      "24    Fast Eddie,I made two of these up when you fir...\n",
      "35    Going out in today's mail:Donald Thompson, Jr....\n",
      "51    Sets going out today to:Andrew BarnardGeorge G...\n",
      "55    Sets going out in today's mail to:Lars Helgese...\n",
      "60    Know you have many responses but would gladly ...\n",
      "62              Thanks Ed, I'm sending you an envelope.\n",
      "77    I don't have a shipping weight, but the post o...\n",
      "Name: FormattedBody, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (reply_match_search['FormattedBody'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk.\n",
    "temp_file = datapath(\"lda.model\")\n",
    "lda_model.save(temp_file)\n",
    "\n",
    "# Load lda.model\n",
    "lda = gensim.models.ldamodel.LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(lda_model.print_topics(num_words = 10), columns = ['Topic', 'Keywords']).to_csv('LDA_vector.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Normalization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalization(col):\n",
    "    nomolized_col = (df[col] - df[col].mean()) / df.loc[:, col].std()\n",
    "    return nomolized_col\n",
    "\n",
    "df['RecencyRate'] = Normalization('RecencyRate')\n",
    "df['AuthorScore'] = Normalization('AuthorScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Author</th>\n",
       "      <th>Like</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Resource</th>\n",
       "      <th>SourceScore</th>\n",
       "      <th>RecencyRate</th>\n",
       "      <th>AuthorScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>President's Column</td>\n",
       "      <td>JANUARY FEBRUARY 20154CIRRUS PILOTAs this is b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Magazine</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.465959</td>\n",
       "      <td>-1.209951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>COPA News</td>\n",
       "      <td>JANUARY FEBRUARY 20156CIRRUS PILOTWith this is...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Magazine</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.465959</td>\n",
       "      <td>-1.209951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Regional News</td>\n",
       "      <td>JANUARY FEBRUARY 201512CIRRUS PILOTby GIL WILL...</td>\n",
       "      <td>GIL WILLIAMSON</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Magazine</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.465959</td>\n",
       "      <td>0.211050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Cirrus Perspective</td>\n",
       "      <td>JANUARY FEBRUARY 201518CIRRUS PILOTIts hard to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Magazine</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.465959</td>\n",
       "      <td>-1.209951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Member Spotlight</td>\n",
       "      <td>JANUARY FEBRUARY 201522CIRRUS PILOTCirrus Pilo...</td>\n",
       "      <td>KIM BLONIGEN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Magazine</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.465959</td>\n",
       "      <td>0.211050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date               Title  \\\n",
       "0  2015-01-01  President's Column   \n",
       "1  2015-01-01           COPA News   \n",
       "2  2015-01-01       Regional News   \n",
       "3  2015-01-01  Cirrus Perspective   \n",
       "4  2015-01-01    Member Spotlight   \n",
       "\n",
       "                                             Content          Author  Like  \\\n",
       "0  JANUARY FEBRUARY 20154CIRRUS PILOTAs this is b...             NaN     0   \n",
       "1  JANUARY FEBRUARY 20156CIRRUS PILOTWith this is...             NaN     0   \n",
       "2  JANUARY FEBRUARY 201512CIRRUS PILOTby GIL WILL...  GIL WILLIAMSON     0   \n",
       "3  JANUARY FEBRUARY 201518CIRRUS PILOTIts hard to...             NaN     0   \n",
       "4  JANUARY FEBRUARY 201522CIRRUS PILOTCirrus Pilo...    KIM BLONIGEN     0   \n",
       "\n",
       "   Comment  Resource  SourceScore  RecencyRate  AuthorScore  \n",
       "0        0  Magazine            3    -0.465959    -1.209951  \n",
       "1        0  Magazine            3    -0.465959    -1.209951  \n",
       "2        0  Magazine            3    -0.465959     0.211050  \n",
       "3        0  Magazine            3    -0.465959    -1.209951  \n",
       "4        0  Magazine            3    -0.465959     0.211050  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input Searching query__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Search Results Ranking\n",
    "\n",
    "- \"Cirrus Parachute First Responders\"\n",
    "- \"Different between AATD and FTD\"\n",
    "- \"Activate vector to final\"\n",
    "- \"Turbo normalize vs turbo charge\"\n",
    "- \"Shooting ILS approach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(search_query):\n",
    "    '''\n",
    "    Get the most important keyword from searching query \n",
    "    '''\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.append('vs')\n",
    "    r = Rake(stopwords = stop_words)\n",
    "    \n",
    "    r.extract_keywords_from_text(search_query)\n",
    "    rank = r.get_ranked_phrases_with_scores()\n",
    "    itemMaxValue = max(rank, key=lambda x: x[0])[0]\n",
    "    \n",
    "    # Find the most important keyword\n",
    "    listOfKeys = list()\n",
    "    for value, key in rank:\n",
    "        if value == itemMaxValue:\n",
    "            listOfKeys.append(key.lower())\n",
    "    \n",
    "    return listOfKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_query = [\"Cirrus Parachute First Responders\", \"Difference between AATD and FTD\", \"Activate vector to final\", \"Turbo normalize vs turbo charge\", \"Shooting ILS approach\"]\n",
    "# extract_keywords(\"Difference between AATD and FTD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_terms = [tree['Title'] for tree in taxonomy_data['data'][1:]]\n",
    "\n",
    "topic_vector = {}\n",
    "for i in range(len(taxonomy_terms)):\n",
    "    topic_info = taxonomy_data['data'][1:][i] # Sub level of all terms other than COPA\n",
    "    title = re.findall(r'Title\\': \\'(.*?)\\'', str(topic_info)) # Extract all children titles under specific term\n",
    "    if len(title) > 1: # If the term is among the bottom level, then just assign itself to the term\n",
    "        topic_vector[taxonomy_terms[i].lower()] = title[1:]\n",
    "    else:\n",
    "        topic_vector[taxonomy_terms[i].lower()] = [taxonomy_terms[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about which fuzzywuzzy method we should use:\n",
    "For each vector in topic_vector: which level we should match to.\n",
    "What kind of vector we want to get in the end? \n",
    "Current version: Combine all the match vectors together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 80  # Maybe adjusted to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare query to each leaf node, if the partial match score is higher than the threshold, then return a combination vector\n",
    "# that contains all words in those leaf nodes.\n",
    "def fuzzywuzzy_match(query,threshold):\n",
    "    \n",
    "    # Generate a list contains all leaf nodes, each sublist contains all words in that leaf node.\n",
    "    leaf_node = list()\n",
    "    for item in topic_vector:\n",
    "        if len(topic_vector[item]) == 1:\n",
    "            leaf_node.append(topic_vector[item][0].split())\n",
    "            \n",
    "    reference_vector = list()\n",
    "    \n",
    "    for node in leaf_node:\n",
    "        query_len = len(query)\n",
    "        node_len = len(node)\n",
    "        \n",
    "        if query_len >= node_len: # If query is longer\n",
    "            for i in range(node_len):\n",
    "                score = fuzz.partial_ratio(node[i],query)\n",
    "                if score > threshold:\n",
    "                    reference_vector.append(node) \n",
    "                    \n",
    "        else: # If leaf node is longer\n",
    "            for j in range(query_len):\n",
    "                score = fuzz.partial_ratio(query[j],node)\n",
    "                if score > threshold:\n",
    "                    reference_vector.append(node)\n",
    "    \n",
    "    reference_vector = list(list(i) for i in set(map(tuple, reference_vector))) # Deduplication\n",
    "    \n",
    "    return reference_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Parachute'],\n",
       " ['Cirrus', 'Airframe', 'Parachute', 'System'],\n",
       " ['Cirrus', 'Approach'],\n",
       " ['Parachute', 'Disreef'],\n",
       " ['Over', 'a', 'Runaway'],\n",
       " ['Reefed', 'Parachute'],\n",
       " ['Rocket', 'and', 'Parachute', 'Extraction'],\n",
       " ['Not', 'Over', 'a', 'Runaway']]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Cirrus Parachute First Responders\"\n",
    "fuzzywuzzy_match(query,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(search_query):\n",
    "    '''\n",
    "    The searching query can be exact matched by taxonomy tree.\n",
    "    '''\n",
    "    if search_query in taxonomy_terms:\n",
    "        taxonomy_vector = topic_vector[search_query.lower()]\n",
    "    return taxonomy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Goodyear', 'Michelin', 'Aero Classic', 'STA']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exact_match('Tire Brands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector that stores all the phrases(more than one word) that should be \n",
    "# token as a whole.\n",
    "def generate_phrases_tokenizer(topic_vector): # Return a list of tuples containing phrases.\n",
    "    all_multiword_tokens = list()\n",
    "    for item in topic_vector: \n",
    "        for phrase in topic_vector[item]:\n",
    "            phrase = phrase.replace('(','') # Eliminate '(' and ')' in the phrase\n",
    "            phrase = phrase.replace(')','')\n",
    "           #phrase = re.sub(r'( \\(.*\\))','',phrase) #exclude all contents in '()'\n",
    "            phrase_lower = phrase.lower() # Convert all characters into lower case.\n",
    "            #phrase_lower.replace('-', ' ')\n",
    "            word_lst = phrase_lower.split()\n",
    "            if len(word_lst) >= 2: \n",
    "                word_tuple = tuple(word_lst)\n",
    "                all_multiword_tokens.append(word_tuple)  \n",
    "    all_multiword_tokens = list(set(all_multiword_tokens)) # Deduplication\n",
    "    tokenizer = MWETokenizer(all_multiword_tokens,separator=' ')\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_match(search_query,tokenizer):\n",
    "    '''\n",
    "    Any words in searching query can be exact matched by taxonomy tree.\n",
    "    '''\n",
    "    # Split input string into words or phrases according to taxonomy tree.\n",
    "    words = tokenizer.tokenize(search_query.split())\n",
    "    taxonomy_vector = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            taxonomy_vector.append(topic_vector[word])\n",
    "        except:\n",
    "            continue\n",
    "    return taxonomy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Turbo']]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_tokenizer = generate_phrases_tokenizer(topic_vector)\n",
    "any_match(\"Turbo normalize vs turbo charge\", phrase_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_match(keywords):\n",
    "    '''\n",
    "    The most important keyword of searching query can be matched by taxonomy tree.\n",
    "    '''\n",
    "    taxonomy_vector = []\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            taxonomy_vector.append(topic_vector[keyword])\n",
    "        except:\n",
    "            continue\n",
    "    return taxonomy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Engine']], ['work', 'engine'])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keywords = extract_keywords(\"Engine doesn't work.\")\n",
    "# keyword_match(keywords), keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_match(search_query):\n",
    "    '''\n",
    "    Extract the topic of searching query by LDA model trained by forum posts.\n",
    "    '''\n",
    "    texts1 = [[word for word in doc.lower().split() if word not in stop_words] for doc in [search_query]]\n",
    "    corpus1 = [id2word.doc2bow(t) for t in texts1]\n",
    "    result = lda_model.get_document_topics(corpus1)\n",
    "    LDA_topic = max(result[0], key=lambda x: x[1])[0]\n",
    "    return LDA_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,\n",
       " (12,\n",
       "  '0.101*\"altitude\" + 0.083*\"high\" + 0.065*\"climb\" + 0.041*\"low\" + 0.034*\"performance\" + 0.032*\"gun\" + 0.030*\"turbo\" + 0.028*\"level\" + 0.028*\"foot\" + 0.027*\"rate\"'))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topic_match('Turbo normalize vs turbo charge'), lda_model.print_topics(num_words = 10)[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
